{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySparkDelayedFlights",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/PySpark/blob/master/PySparkDelayedFlights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WcC7uUz7S4u",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3\n",
        "\n",
        "\n",
        "Check for the latest version of Spark at https://pypi.org/project/pyspark/#history\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD5KzNGA7RZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setting up Apache Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "print(\"If no error - everything is working\")\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv55s8h_8hev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Point Colaboratory to your Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k67iOVTXEfEg",
        "colab_type": "text"
      },
      "source": [
        "Get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kztd3FhwEAa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download datasets directly to your Google Drive \"Colab Datasets\" folder\n",
        "\n",
        "import requests\n",
        "\n",
        "# 2007 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "with open(\"/content/gdrive/My Drive/2007.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7M19HgCEbde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2008 data\n",
        "\n",
        "file_url = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\n",
        "\n",
        "r = requests.get(file_url, stream = True) \n",
        "\n",
        "with open(\"/content/gdrive/My Drive/2008.csv.bz2\", \"wb\") as file: \n",
        "\tfor block in r.iter_content(chunk_size = 1024): \n",
        "\t\tif block: \n",
        "\t\t\tfile.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1RFC0UzFEX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tools we need to connect to the Spark server, load our data,\n",
        "# clean it and prepare it\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "# Set up constants\n",
        "CSV_2007= \"/content/gdrive/My Drive/2007.csv.bz2\" \n",
        "CSV_2008= \"/content/gdrive/My Drive/2008.csv.bz2\"\n",
        "APP_NAME = \"Flight Delays\"\n",
        "SPARK_URL = \"local[*]\"\n",
        "RANDOM_SEED = 141109\n",
        "TRAINING_DATA_RATIO = 0.7\n",
        "RF_NUM_TREES = 8\n",
        "RF_MAX_DEPTH = 4\n",
        "RF_NUM_BINS = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0V8_n8LFJAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Connect to the Spark server\n",
        "\n",
        "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
        "\n",
        "# Load datasets\n",
        "\n",
        "df_2007 = spark.read.options(header=\"true\",inferschema = \"true\").csv(CSV_2007)\n",
        "df_2008 = spark.read.options(header=\"true\",inferschema = \"true\").csv(CSV_2008)\n",
        "\n",
        "# We concatenate both datasets\n",
        "\n",
        "df = df_2007.unionAll(df_2008)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-raO-RCFzje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj-kmimfGHoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)for c in df.columns]).toPandas().to_dict(orient='records')\n",
        "print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRGyVWc-HnlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(df.CancellationCode)\n",
        "df = df.na.drop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gykHxAmtH2tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq1sdujBH566",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.select('Cancelled').distinct().rdd.map(lambda r: r[0]).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTswmtlID66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_cols = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'FlightNum', 'Distance', 'Diverted']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K52xVd6EIb4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").transform(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9K47_UIfF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.select(\"Cancelled\", \"features\").show(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwjvRtYxIqZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a labelIndexer\n",
        "labelIndexer = StringIndexer(inputCol=\"Cancelled\", outputCol=\"indexedLabel\").fit(df)\n",
        "\n",
        "# Generate the indexed feature vector\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
        "    \n",
        "# Split the data into training and tests sets\n",
        "(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
        "\n",
        "# Train the RandomForest model\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
        "\n",
        "# Chain indexers and the forest models in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma_tQZWDItgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model\n",
        "model = pipeline.fit(trainingData)\n",
        "# Make predictions\n",
        "predictions = model.transform(testData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihapr9xuIwKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
        "print(f\"Accuracy = {accuracy:g}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}